<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://timegarage.github.io</id>
    <title>少数派报告</title>
    <updated>2019-12-02T13:10:59.068Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://timegarage.github.io"/>
    <link rel="self" href="https://timegarage.github.io/atom.xml"/>
    <subtitle>Minority Report</subtitle>
    <logo>https://timegarage.github.io/images/avatar.png</logo>
    <icon>https://timegarage.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, 少数派报告</rights>
    <entry>
        <title type="html"><![CDATA[模型评估 - 百面机器学习]]></title>
        <id>https://timegarage.github.io/post/Note-ModelEvaluation</id>
        <link href="https://timegarage.github.io/post/Note-ModelEvaluation">
        </link>
        <updated>2019-11-16T14:13:44.000Z</updated>
        <content type="html"><![CDATA[<h1 id="模型评估">模型评估</h1>
<h3 id="评估指标的局限性">评估指标的局限性</h3>
<ul>
<li>
<p>准确率的局限性</p>
<blockquote>
<p>当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。</p>
</blockquote>
</li>
<li>
<p>精确率与召回率的权衡</p>
<ul>
<li>
<p>P-R曲线（Precision-&gt; 查准率 Recall -&gt; 查全率）</p>
</li>
<li>
<p>F1 score</p>
</li>
<li>
<p>ROC曲线</p>
</li>
</ul>
</li>
<li>
<p>平方根误差的意外</p>
<blockquote>
<p>在实际问题中，如果存在个别偏离程度非常大的离群点（Outlier）时，即使离群点数量非常少，也会让 RMSE 指标变得很差。</p>
</blockquote>
<ul>
<li>
<p>RMSE</p>
</li>
<li>
<p>解决方案</p>
<ul>
<li>
<p>如果确认离群点为噪声点 -&gt; 数据预处理阶段把噪声过滤掉</p>
</li>
<li>
<p>如果不认为这些离群点为噪声点 -&gt; 提高模型的预测能力 -&gt; 将离群点产生的机制建模进去</p>
</li>
<li>
<p>寻找一个更合适的评估指标来评估模型</p>
<ul>
<li>MAPE</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>相比 RMSE，MAPE 相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。</p>
</blockquote>
<h3 id="roc曲线">ROC曲线</h3>
<ul>
<li>
<p>什么是ROC曲线</p>
<blockquote>
<p>ROC 曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）</p>
</blockquote>
</li>
<li>
<p>如何绘制ROC曲线</p>
<ul>
<li>截断点/阈值</li>
</ul>
</li>
<li>
<p>如何计算AUC</p>
<ul>
<li>评价：AUC越大，说明分类器性能越好。</li>
</ul>
</li>
<li>
<p>ROC曲线相比P-R曲线有什么特点</p>
</li>
</ul>
<h3 id="余弦距离的应用">余弦距离的应用</h3>
<ul>
<li>余弦相似度与欧式距离
<ul>
<li>相对差异 -&gt; 余弦相似度</li>
<li>绝对差异 -&gt; 欧式距离</li>
</ul>
</li>
<li>余弦距离是否是一个**<u>严格定义的距离</u>**
<ul>
<li>✅ 正定性
<ul>
<li>证明：定义</li>
</ul>
</li>
<li>✅对称性
<ul>
<li>证明：定义</li>
</ul>
</li>
<li>❌三角不等式
<ul>
<li>证明：单位元三个紧挨着的点 / 等腰直角三角形</li>
</ul>
</li>
</ul>
</li>
<li>KL（Kullback-Leibler Divergence）距离（又名相对熵）
<ul>
<li>✅正定性</li>
<li>❌对称性</li>
<li>❌三角不等式</li>
</ul>
</li>
</ul>
<h3 id="ab测试的陷阱">A/B测试的陷阱</h3>
<ul>
<li>为什么在对模型进行过充分的离线评估之后，还要进行在线的A/B测试？
<ul>
<li>离线评估无法完全消除模型过拟合的影响</li>
<li>离线评估无法完全还原线上的工程环境</li>
<li>线上系统的某些商业指标在离线评估中无法计算</li>
</ul>
</li>
<li>如何进行线上A/B测试？
<ul>
<li>实验组 ⬅️ 新模型</li>
<li>对照组 ⬅️ 旧模型</li>
<li>注意点：样本的独立性和采样方式的无偏性</li>
</ul>
</li>
<li>如何划分实验组和对照组</li>
</ul>
<h3 id="模型评估的方法">模型评估的方法</h3>
<ul>
<li>
<p>Holdout检验</p>
</li>
<li>
<p>交叉检验</p>
</li>
<li>
<p>自助法</p>
<blockquote>
<p>当样本数很大时，大约有 36.8% 的样本从未被选择过，可作为验证集。</p>
</blockquote>
</li>
</ul>
<h3 id="超参数有哪些调优方法">超参数有哪些调优方法</h3>
<ul>
<li>
<p>网格搜索</p>
<blockquote>
<p>通过查找搜索范围内的所有的点来确定最优值</p>
</blockquote>
</li>
<li>
<p>随机搜索</p>
</li>
<li>
<p>贝叶斯优化算法</p>
<blockquote>
<p>首先根据先验分布，假设一个搜集函数；然后，每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；最后，算法测试由后验分布给出的全局最值最可能出现的位置的点。</p>
</blockquote>
</li>
</ul>
<h3 id="过拟合与欠拟合">过拟合与欠拟合</h3>
<ul>
<li>
<p>过拟合和欠拟合具体是指什么现象</p>
<ul>
<li>
<p>过拟合</p>
<blockquote>
<p>过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。</p>
</blockquote>
</li>
<li>
<p>欠拟合</p>
<blockquote>
<p>欠拟合指的是模型在训练和预测时表现都不好的情况。</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>降低过拟合和欠拟合风险的方法有哪些</p>
<ul>
<li>
<p>降低过拟合风险的方法</p>
<ul>
<li>
<p>获取更多的训练数据</p>
<blockquote>
<p>使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减小噪声的影响。</p>
</blockquote>
<p>图像：平移、旋转、缩放、使用生成对抗网络合成新的训练数据</p>
</li>
<li>
<p>降低模型复杂度</p>
<blockquote>
<p>在数据较少时，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。</p>
</blockquote>
<p>例如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。</p>
</li>
<li>
<p>正则化方法</p>
<blockquote>
<p>避免权值过大带来的过拟合风险</p>
</blockquote>
</li>
<li>
<p>集成学习</p>
<blockquote>
<p>将多个模型集成到一起，降低单一模型的过拟合风险。</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>降低欠拟合的方法</p>
<ul>
<li>
<p>添加新特征</p>
<blockquote>
<p>当特征不足或者现有特征与样本标签的相关性不强时，容易出现欠拟合。</p>
</blockquote>
<ul>
<li>因子分解机 FM</li>
<li>梯度提升决策树 GBDT</li>
<li>Deep-Crossing</li>
</ul>
</li>
<li>
<p>增加模型复杂度</p>
<ul>
<li>线性模型：添加高次项</li>
<li>神经网络模型：增加网络层数、神经元个数</li>
<li>减小正则化系数</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[特征工程 - 百面机器学习]]></title>
        <id>https://timegarage.github.io/post/Note-FeatureEngineering</id>
        <link href="https://timegarage.github.io/post/Note-FeatureEngineering">
        </link>
        <updated>2019-11-15T14:36:27.000Z</updated>
        <content type="html"><![CDATA[<h1 id="特征工程">特征工程</h1>
<h3 id="特征归一化">特征归一化</h3>
<ul>
<li>目的：消除量纲影响</li>
<li>数值型特征归一化方法
<ul>
<li>线性函数归一化</li>
<li>零均值归一化</li>
</ul>
</li>
<li>归一化对梯度下降法收敛速度的影响</li>
</ul>
<h3 id="类别型特征">类别型特征</h3>
<ul>
<li>
<p>序号编码</p>
<blockquote>
<p>序号编码通常用于处理类别间具有大小关系的数据</p>
</blockquote>
</li>
<li>
<p>独热编码</p>
<blockquote>
<p>独热编码通常用于处理类别间不具有大小关系的特征</p>
</blockquote>
<ul>
<li>使用稀疏向量来节省空间</li>
<li>配合特征选择降低维度
<ul>
<li>高维度特征带来的问题
<ul>
<li>KNN下，两点距离很难度量</li>
<li>LR下，参数数量增加，容易引起过拟合</li>
<li>只有部分维度对分类、预测有帮助</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>二进制编码</p>
</li>
<li>
<p>扩展</p>
<ul>
<li>Helmert Contrast</li>
<li>Sum Contrast</li>
<li>Polynomial Contrast</li>
<li>Backward Difference Contrast</li>
</ul>
</li>
</ul>
<h3 id="高维组合特征的处理">高维组合特征的处理</h3>
<ul>
<li>目的：提高对复杂关系的拟合能力</li>
<li>组合</li>
<li>分解
<ul>
<li>矩阵分解</li>
</ul>
</li>
</ul>
<h3 id="组合特征">组合特征</h3>
<ul>
<li>基于决策树的特征组合寻找方法
<ul>
<li>梯度提升决策树</li>
</ul>
</li>
</ul>
<h3 id="文本表示模型">文本表示模型</h3>
<blockquote>
<p>文本是一类非常重要的非结构化数据</p>
</blockquote>
<ul>
<li>
<p>词袋模型和N-gram模型</p>
<ul>
<li>
<p>TF-IDF</p>
<ul>
<li>
<p>TF-IDF(<em>t</em>,<em>d</em>)=TF(<em>t</em>,<em>d</em>)×IDF(<em>t</em>)</p>
<blockquote>
<p>其中 TF(<em>t</em>,<em>d</em>)为单词<em>t</em>在文档<em>d</em>中出现的频率，IDF(<em>t</em>)是逆文档频率，用来衡量单词<em>t</em>对表达语义所起的重要性。</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>N-gram</p>
<blockquote>
<p>通常，可以将连续出现的<em>n</em>个词（<em>n</em>≤<em>N</em>）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成 N-gram 模型。</p>
</blockquote>
</li>
<li>
<p>词干抽取 Word Stemming</p>
<blockquote>
<p>将不同词性的单词统一成同一词干的形式。</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>主题模型</p>
<ul>
<li>主题分布特性</li>
</ul>
</li>
<li>
<p>词嵌入与深度学习模型</p>
<ul>
<li>深度学习 》自动特征工程</li>
<li>CNN与RNN能够更好的对文本进行建模，抽取出一些高层的语义特征。</li>
</ul>
</li>
</ul>
<h3 id="word2vec">Word2Vec</h3>
<blockquote>
<p>CBOW 的目标是根据上下文出现的词语来预测当前词的生成概率；而 Skip-gram 是根据当前词来预测上下文中各词的生成概率;</p>
</blockquote>
<ul>
<li>网络结构
<ul>
<li>Continues Bag of Words （CBOW）</li>
<li>Skip-gram</li>
<li>层
<ul>
<li>输入层 -&gt; 每个词由独热编码方式表示</li>
<li>映射层</li>
<li>输出层</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="图像数据不足时的处理方法">图像数据不足时的处理方法</h3>
<ul>
<li>基于模型的方法 -&gt; 降低过拟合的风险
<ul>
<li>简化模型</li>
<li>添加约束项（如L1/L2正则项）</li>
<li>集成学习</li>
<li>Dropout</li>
</ul>
</li>
<li>基于数据的方法
<ul>
<li>数据扩充
<ul>
<li>随机旋转、平移、缩放、裁剪、填充、左右翻转</li>
<li>添加噪声</li>
<li>颜色变换</li>
<li>改变亮度、清晰度、对比度、锐度</li>
</ul>
</li>
<li>迁移学习</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
</feed>